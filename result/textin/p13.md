<!-- This version posted 2025-05-27. -->

# Physical models realizing the transformer architecture of large language models

Zeqian Chen*

Wuhan Institute of Physics and Mathematics, IAPM,Chinese Academy of Sciences,

West District 30, Xiao-Hong-Shan, Wuhan 430071,China

The introduction of the transformer architecture in 2017 (cf.[11]) marked the most striking ad-vancement in natural language processing. The transformer is a model architecture relying entirely on an attention mechanism to draw global dependencies between input and output. However, we believe there is a gap in our theoretical understanding of what the transformer is, and why it works physically. In this paper, from a physical perspective on modern chips, we construct physical mod-els in the Fock space over the Hilbert space of tokens realizing large language models based on a transformer architecture as open quantum systems. Our physical models underlie the transformer architecture for large language models.

## PACS numbers:

Large language models (LLMs for short) are based on deep neural networks (DNNs) (cf.[5]), and a common characteristic of DNNs is their compositional nature: da-ta is processed sequentially,layer by layer, resulting in a discrete-time dynamical system. The introduction of the transformer architecture in 2017 marked the most strik-ing advancement in terms of DNNs (cf.[11]). Indeed, the transformer is a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. At each step, the model is auto-regressive,consuming the previously generated symbols as additional input when generating the next. The transformer has achieved great success in natural language processing (cf.[15] and refer-ences therein).

The transformer has a modularization framework and is constructed by two main building blocks: self-attention and feed-forward neutral networks. Self-attention is an attention mechanism (cf.[1]) relating different positions of a single sequence in order to compute a representation of the sequence. In line with successful large language models,one often focuses on the decoder-only setting of the transformer, where the model iteratively predicts the next tokens based on a given sequence of tokens. This procedure is coined autoregressive since the prediction of new tokens is only based on previous tokens. Such con-ditional sequence generation using autoregressive trans-formers is referred to as the transformer architecture. However, despite its meteoric rise within deep learning, we believe there is a gap in our theoretical understanding of what the transformer is, and why it works physically (cf.[6]).

To the best of our knowledge, physical models for the transformer architecture of large language models are usually described by using systems of mean-field inter-acting particles (cf. [4,13] and references therein),i.e., large language models are regarded as classical statisti-cal systems. However, since modern chips process da-ta through controlling the flow of electric current,i.e., the dynamics of largely many electrons, so they should be regarded as quantum statistical ensembles and open quantum systems from a physical perspective (cf.[2,12]). In this paper, we construct physical models in the Fock space over the Hilbert space of tokens realizing large lan-guage models based on a transformer architecture as open quantum systems.

*Electronic address: chenzeqian@hotmail.com

In the transformer architecture of a large language model S, we assume the finite set T of tokens in S has been embedded inR,where d is called the em-bedding dimension, so we identify each tET with one of finitely-many vectors x in Rd..We assume that the structure (positional information, adjacency informa-tion, etc) is encoded in these vectors. A finite sequence {xi}i=1 of tokens is called a text for S, simply denoted byT=x1x2⋯xnor(x1x2⋯xn),where n is called the length of the text T. We write [n]={1,2,⋯,n}for an integer n. For a Hilbert space K,we use C(K)and S(K) respectively to denote the set of all linear bounded operators and the set of all density operators in K.

Recall that a self-attention layer SelfAtt with an at-tention mechanism(WQ,WK,WV) in the transformer architecture is the only layer that combines different to-kens,whereWQandWK are twod′×real matrixes (i.e., the query and key matrixes) andWVis thed×d real matrix (called the value matrix) such thatWVx∈T for∈T. Let us denote the input text to the layer by X={xi}i=1nFor eachi∈[n],letting

$$s_{i}=\frac {1}{\sqrt {d}}\left\langle W^{Q}x_{n},W^{K}x_{i}\right\rangle ,\quad \forall i\in [n],$$

we can interpretS(n)={si}i=1n as similarities between the n th token xn(i.e., the query) and the other tokens (i.e., keys). The softmax layer is given by

$$\text {softmax}\left(S^{(n)}\right)_{i}=\frac {e^{s_{i}}}{\sum _{j=1}^{n}e^{s_{j}}}\quad \quad \forall i\in [n]$$

which can be interpreted as the probability for the n-th query to “attend" to the i-thkey. Then the self-attention

<!-- <table border="1" ><tr>
<td></td>
</tr><tr>
<td>I A 9<br>9<br>7</td>
</tr><tr>
<td>0<br>0<br>.<br>S</td>
</tr><tr>
<td>O S 7<br>0<br>Z :<br>A</td>
</tr><tr>
<td>I X e u I</td>
</tr><tr>
<td>O</td>
</tr><tr>
<td></td>
</tr></table> -->

<!-- 2 -->

<!-- This version posted 2025-05-27. -->

layer SelfAtt is defined by

$$\text {SelfAtt}(X)_{n}=\sum _{i=1}^{n}\text {softmax}\left(S^{(n)}\right)_{i}W^{V}x_{i},\tag{1}$$

indicating that the outputWVxioccurs with the prob-ability softmax(S(n))i,which is often referred to as the values of the tokenxi.Note that(WQ,WK,WV) are trainable parameters in the transformer architecture.

In the same block, a feed-forward neural network FFN is then applied toWVxi'ssuch thatyi=FFN(WVxi) with the probability softmax:(S(n))i for eachi∈[n],and so the output isxn+1=i=FFNoSelfAtt({xi}i=1n)for somei∈[n]. One can then apply the same operations to the extended sequencex1x2⋯xnxn+1 in a next block, obtainingx+2=FFN′∘SlfA′({xi}i=1+1),to iterative-ly compute further tokens (there is usually a stopping criterion based on a special token).

Typically,a transformer of depth L is defined by a com-position of L blocks,denoted by TransfL,consisting of L self-attention maps {SelfAtte}t=1 and L feed-forward neural networks{FFN\ell}\ell=1L,i.e.

$$\text {Transf}_{L}=\left(\mathrm {FFN}_{L}\circ \text {SelfAtt}_{L}\right)\circ \cdots \circ \left(\mathrm {FFN}_{1}\circ \text {SelfAtt}_{1}\right)(2)$$

where the indices of the layers SelfAtt and FFN in (2) indicate the use of different trainable parameters in each of the block. Then, given an input textT=x1⋯x, TransfLgenerates a textyi1⋯yiL with the joint proba-bility

$$P_{T}\left(y_{i_{1}},\cdots ,y_{i_{L}}\right)=\text {softmax}\left(S_{1}^{(n)}\right)_{i_{1}}\cdots \text {softmax}\left(S_{L}^{(n+L-1)}\right)_{i_{L}}$$

where softmax(S(n+\ell-1)\ell)i\ellis given by the attention mechanism(W\ellQ,W\ellK,W\ellV) in the e-th building block for each\ell=1,⋯,L.

Now, we are ready to construict a physical model real-izing the transformer architecture (2) for LLMs. To this end, consider a large language model S with the set T of N tokens embedded in Rd. Let h be the Hilbert space with{|x&gt;: x∈T}being an orthogonal basis,and we identityx=|x〉 forx∈T.Let HHI=F(h) be the Fock space over h,that is,

$$\mathcal {F}(\mathbf {h})=\mathbb {C}\oplus \bigoplus _{n=1}^{\infty }\mathbf {h}^{\otimes n}$$

whereh⊗n is the n-fold tensor product of h (cf.[9]) In what follows, for the sake of convenience, we involve the finite Fock space

$$\mathbb {H}=\mathcal {F}^{(M)}(\mathbf {h})=\mathbb {C}\oplus \bigoplus _{n=1}^{M}\mathbf {h}^{\otimes n}$$

where M is an integer such thatM»N. Note that an operatorA()=A1⊗⋯⊗A∈L(⊗)forAj∈L(h) satisfies that for allh(n)=h1⊗⋯⊗hn∈h⊗n

$$Ah^{(n)}=\left(A_{1}h_{1}\right)\otimes \cdots \otimes \left(A_{n}h_{n}\right)\in \mathbf {h}^{\otimes n}$$

and in particular, ifρi∈S(h)for iE[n],then ρ()=ρ1⊗⋯⊗ρ∈S(h⊗).Given α∈C and a sequenceA(n)∈L(h⊗n) forn∈[M],,the operator dig(α,A(1),⋯,A(M))∈L(H)is defined by

$$\text {diag}\left(α,A^{(1)},\cdots ,A^{(M)}\right)\mathrm {h}^{(M)}=\left(αc,A^{(1)}h^{(1)},\cdots ,A^{(M)}h^{(M)}\right)$$

for everyh(M)=(c,h(1),⋯,h(M))∈H.Inpaticula,if ρ(n)∈S(h⊗n), n

$$ρ^{(M)}=\text {diag}\left(0,0^{(1)},\cdots ,0^{(n-1)},ρ^{(n)},0^{(n+1)},\cdots ,0^{(M)}\right)\in \mathcal {S}(\mathbb {H})$$

where0(i)denotes the zero operator inh⊗ifori≥1

It suffices to construct a physical model in the Fock spaceH=F(M)(h)(M»L)for a transformer TransfL (2) with a composition of L blocks, consisting of L self-attention maps{SelAtt\ell=(W\ellQW\ellKW\ellV)}\ell=1 and L feed-forward neural networks{FFN\ell}\ell=1LPrecisely,let us denote the input text to the layer byT={xi}i=1n.As noted above,for\ell=1,⋯,L,one has

$$\mathrm {FFN}_{\ell }\circ \text {SelfAtt}_{\ell }(T)=\sum _{i=1}^{n+\ell -1}\text {softmax}\left(S_{\ell }^{(n+\ell -1)}\right)_{i}\mathrm {FFN}_{\ell }\left(W^{V_{\ell }}x_{i}\right)$$

whereS\ell(n+\ell-1)={si(\ell)}i=1n+\ell-1and

$$s_{i}^{(\ell )}=\frac {1}{\sqrt {d}}\left\langle W^{Q_{\ell }}x_{n+\ell -1},W^{K_{\ell }}x_{i}\right\rangle\quad \quad \forall i\in [n+\ell -1].$$

A physical model needed to construct for TransfL consist of an inputρ(t0) and a sequence of quantum operations {E(t\ell,t0)}\ell=1L in the Fock space HI(cf.[7]),wheret0&lt; t1&lt;⋯&lt;tL. We show how to construct this model step by step as follows.

To this end, we denote byΩ={⋄}∪Tand by2Ω the

<!-- <table border="1" ><tr>
<td></td>
</tr><tr>
<td>I A 9<br>9<br>7</td>
</tr><tr>
<td>0<br>0<br>.<br>S O S 7</td>
</tr><tr>
<td>0<br>Z :</td>
</tr><tr>
<td>A I X e u I O</td>
</tr></table> -->

<!-- 3 -->

<!-- This version posted 2025-05-27. -->

set of all subsets of Ω, and writeD=({ω}:ω∈Ω).At is given as first,for an input textT=x1⋯xn, the input state ρT

$$ρ_{T}=ρ\left(t_{0}\right)=\text {diag}\left(0,0^{(1)},\cdots ,0^{(n-1)},\left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \cdots \otimes \left|x_{n}\right\rangle \left\langle x_{n}\right|,0^{(n+1)},\cdots \right)\in \mathcal {S}(\mathbb {H})$$

Then there is a quantum operationE(t1,t0) in HI depending only on the attention mechanism (W1Q,W1K,W1V)and FFN1such that (see Appendix for the details)

$$\mathcal {E}\left(t_{1},t_{0}\right)ρ\left(t_{0}\right)$$

$$=\sum _{i=1}^{n}\text {softmax}\left(S_{1}^{(n)}\right)_{i}\text {diag}\left(0,0^{(1)}\cdots ,0^{(n)},\left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \cdots \otimes \left|x_{n}\right\rangle \left\langle x_{n}\right|\otimes \left|y_{i}^{(1)}\right\rangle \left\langle y_{i}^{(1)}\right|,0^{(n+2)},\cdots \right),\tag{3}$$

whereyi(1)=FFN1(WV1xi)and{yi(1)}i=1n⊂{|x〉:x∈T}.DefineX1:2Ω↦E(H)by

$$X_{1}(\{\diamond \})=\text {diag}\left(1,I_{\mathbf {h}},\cdots ,I_{\mathbf {h}}^{\otimes n},0^{(n+1)},I_{\mathbf {h}^{\otimes (n+2)}},\cdots \right)$$

and for every x∈T

$$X_{1}(\{x\})=\text {diag}\left(0,0^{(1)},\cdots ,0^{(n)},\underbrace {I_{\mathbf {h}}\otimes \cdots \otimes I_{\mathbf {h}}}_{n}\otimes |x\rangle \langle x|,0^{(n+2)},\cdots \right)$$

Making a measurement(X1,D)at timet1,we obtain an outputyi(1)with probability softmax(S1(n))iand,according to the von Neumann-Lüders reduction postulate (cf.[7]), the appropriate density operator to use for any further calculation is

$$ρ_{\mathrm {red}}\left(t_{1}\right)_{i}=\frac {E_{i}^{(1)}ρ\left(t_{1}\right)E_{i}^{(1)}}{\text {Tr}\left[E_{i}^{(1)}ρ\left(t_{1}\right)\right]}$$

$$=\text {diag}\left(0,0^{(1)}\cdots ,0^{(n)},\left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \cdots \otimes \left|x_{n}\right\rangle \left\langle x_{n}\right|\otimes \left|y_{i}^{(1)}\right\rangle \left\langle y_{i}^{(1)}\right|,0^{(n+2)},\cdots \right),$$

for everyi∈[n],whereρ(t1)=E(t1,t0)ρ(t0),and

$$E_{i}^{(1)}=\text {diag}\left(0,0^{(1)},\cdots ,0^{(n)},\underbrace {I_{\mathbf {h}}\otimes \cdots \otimes I_{\mathbf {h}}}_{n}\otimes \left|y_{i}^{(1)}\right\rangle \left\langle y_{i}^{(1)}\right|,0^{(n+2)},\cdots \right)$$

Next, there is a quantum operationE(t2,t0)in H depending only on the attention mechanism (W2Q,W2K,W2V)and FFN2at time t2such that (see Appendix again)

$$\mathcal {E}\left(t_{2},t_{0}\right)ρ_{\mathrm {red}}\left(t_{1}\right)_{i_{1}}=\sum _{i_{2}=1}^{n+1}\text {softmax}\left(S_{2}^{(n+1)}\right)_{i_{2}}\tag{4}\quad x\text {diag}\left(0,0^{(1)},\cdots ,0^{(n+1)},\left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \cdots \otimes \left|x_{n}\right\rangle \left\langle x_{n}\right|\otimes \right.\quad \left|y_{i_{1}}^{(1)}\right\rangle \left\langle y_{i_{1}}^{(1)}\right|\otimes \left|y_{i_{2}}^{(2)}\right\rangle \left\langle y_{i_{2}}^{(2)}\right|,0^{(n+3)},\cdots )$$

fori1∈[n],whereyi2(2)=FFN2(WV2xi2)(with.xn+1=yi1(1))and{yi(2)}i=1n+1⊂{|x〉:x∈T}.DefineX2:2Ω↦E(H) by

$$X_{2}(\{\diamond \})=\text {diag}\left(1,I_{\mathbf {h}},\cdots ,I_{\mathbf {h}}^{\otimes (n+1)},0^{(n+2)},I_{\mathbf {h}^{\otimes (n+3)}},\cdots \right)$$

and for everyx∈T

$$X_{2}(\{x\})=\text {diag}\left(0,0^{(1)},\cdots ,0^{(n+1)},\underbrace {I_{\mathbf {h}}\otimes \cdots \otimes I_{\mathbf {h}}}_{n+1}\otimes |x\rangle \langle x|,0^{(n+3)},\cdots \right)$$

Making a measurement(X2,D)at time t2,we obtain an output yi2(2) with probability softmax(S2(n+1))i2 and the appropriate density operator to use for any further calculation is

$$ρ_{\mathrm {red}}\left(t_{2}\right)_{i_{1},i_{2}}=\frac {E_{i_{2}}^{(2)}ρ\left(t_{2}\right)_{i_{1}}E_{i_{2}}^{(2)}}{\text {Tr}\left[E_{i_{2}}^{(2)}ρ\left(t_{2}\right)_{i_{1}}\right]}\quad =\text {diag}\left(0,0^{(1)},\cdots ,0^{(n+1)},\left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \cdots \otimes \left|x_{n}\right\rangle \left\langle x_{n}\right|\otimes \left|y_{i_{1}}^{(1)}\right\rangle \left\langle y_{i_{1}}^{(1)}\right|\otimes \left|y_{i_{2}}^{(2)}\right\rangle \left\langle y_{i_{2}}^{(2)}\right|,0^{(n+3)},\cdots \right)$$

<!-- <table border="1" ><tr>
<td></td>
</tr><tr>
<td>I A 9<br>9<br>7</td>
</tr><tr>
<td>0<br>0<br>.<br>S</td>
</tr><tr>
<td>O S</td>
</tr><tr>
<td>7<br>0<br>Z :</td>
</tr><tr>
<td>A I X e u I</td>
</tr><tr>
<td>O</td>
</tr></table> -->

<!-- 4 -->

for eachi2∈[n+1],whereρ(t2)i1=E(t2,t0)ρred(t1)i1 and

$$E_{i_{2}}^{(2)}=\text {diag}\left(0,0^{(1)},\cdots ,0^{(n+1)},\underbrace {I_{\mathbf {h}}\otimes \cdots \otimes I_{\mathbf {h}}}_{n+1}\otimes \left|y_{i_{2}}^{(2)}\right\rangle \left\langle y_{i_{2}}^{(2)}\right|,0^{(n+3)},\cdots \right).$$

Step by step, we obtain a physical model{E(t\ell,t0)}\ell=1L with the input stateρT=ρ(t0) as given an input text T=x1⋯xn such that a text (yi1(1),yi2(2),⋯,yiL(L)) is generated with the probability

$$\mathrm {P}_{T}\left(y_{i_{1}}^{(1)},y_{i_{2}}^{(2)},\cdots ,y_{i_{L}}^{(L)}\right)\quad =\text {softmax}\left(S_{1}^{(n)}\right)_{i_{1}}\cdots \text {softmax}\left(S_{L}^{(n+L-1)}\right)_{i_{L}},$$

within the sequential measurement(X1,D),⋯,(XL,D) Thus, the physical model so constructed realizes the transformer architecture TransfL

A physical model for the transformer with a multi-headed attention (cf.[11]) can be constructed in a simi-lar way. Also, we can construct physical models for the transformer of more complex structure (cf.[14] and refer-ence therein). We omit the details.

**Example.** LetT={x0,x1}be the set of two tokens embedded inR2such thatx0=(1,0) andx1=(0,1). Thenh=C2 with the standard basis|0〉=|x0〉 and |1〉=|x1〉.LetH=F(6)(C2). Assume an input text T=(x0,x1,x0). The input state ρT is then given by

$$ρ_{T}=ρ\left(t_{0}\right)=\text {diag}\left(0,0^{(1)},0^{(2)},\left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \left|x_{0}\right\rangle \left\langle x_{0}\right|,0^{(4)},0^{(5)},0^{(6)}\right)$$

IfW1Q=W1V=FFN1=IandW1K=σx in R2, an associated physical operation E(t1,t0)at timet1 satisfies

$$\mathcal {E}\left(t_{1},t_{0}\right)ρ\left(t_{0}\right)=\frac {2}{e+2}\text {diag}\left(0,0^{(1)},0^{(2)},0^{(3)},\left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{0}\right\rangle \left\langle x_{0}\right|,0^{(5)},0^{(6)}\right)\quad +\frac {e}{e+2}\text {diag}\left(0,0^{(1)},0^{(2)},0^{(3)},\left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{1}\right\rangle \left\langle x_{1}\right|,0^{(5)},0^{(6)}\right)$$

By measurement, we obtainx0with probability $\frac {2}{e+2}$ and obtainx1 with probability $\frac {e}{e+2}$ ,while

$$ρ_{\text {red}}\left(t_{1}\right)_{0}=\text {diag}\left(0,0^{(1)},0^{(2)},0^{(3)},\left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{0}\right\rangle \left\langle x_{0}\right|,0^{(5)},0^{(6)}\right)$$

$$ρ_{\mathrm {red}}\left(t_{1}\right)_{1}=\text {diag}\left(0,0^{(1)},0^{(2)},0^{(3)},\left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{1}\right\rangle \left\langle x_{1}\right|,0^{(5)},0^{(6)}\right)$$

IfW2Q=W2K=FFN2=IandW2V=σxinR2,an associated quantum operationE(t2,t0) at timet2satisfies

$$\mathcal {E}\left(t_{2},t_{0}\right)ρ_{\mathrm {red}}\left(t_{1}\right)_{0}\quad =\frac {1}{3e+1}\left(0,0^{(1)},0^{(2)},0^{(3)},0^{(4)},\left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{0}\right\rangle \left\langle x_{0}\right|,0^{(6)}\right)+\frac {3e}{3e+1}\left(0,0^{(1)},0^{(2)},0^{(3)},0^{(4)},\left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{1}\right\rangle \left\langle x_{1}\right|,0^{(6)}\right)$$

and

$$\mathcal {E}\left(t_{2},t_{0}\right)ρ_{\mathrm {red}}\left(t_{1}\right)_{1}\quad =\frac {e}{e+1}\left(0,0^{(1)},0^{(2)},0^{(3)},0^{(4)},\left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \left|x_{0}\right\rangle \left\langle x_{0}\right|,0^{(6)}\right)+\frac {1}{e+1}\left(0,0^{(1)},0^{(2)},0^{(3)},0^{(4)},\left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \left|x_{0}\right\rangle \left\langle x_{0}\right|\otimes \left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \left|x_{1}\right\rangle \left\langle x_{1}\right|,0^{(6)}\right)$$

By measurement at time x2 ,whenx0 occurs at t1,we ob-tain x0 with probability $\frac {1}{3e+1}$  and obtain x1 with prob-ability $\frac {3e}{3e+1}$ whenx1 occurs at t1,we obtain x0 with probability $\frac {e}{e+1}$  and obtain x1 with probability $\frac {1}{e+1}$ 

<!-- This version posted 2025-05-27. -->

<!-- <table border="1" ><tr>
<td>I A 9<br>9<br>7<br>0<br>0<br>.<br>S O S 7<br>0<br>Z :<br>A X I e u I O</td>
</tr></table> -->

<!-- 5 -->

Hence, we obtain the joint probability distributions:

$$P_{T}\left(x_{0},x_{0}\right)=\frac {2}{e+2}\frac {1}{3e+1}=\frac {2}{(e+2)(3e+1)},$$

$$P_{T}\left(x_{0},x_{1}\right)=\frac {2}{e+2}\frac {3e}{3e+1}=\frac {6e}{(e+2)(3e+1)},$$

$$P_{T}\left(x_{1},x_{0}\right)=\frac {e}{e+2}\frac {e}{e+1}=\frac {e^{2}}{(e+2)(e+1)},$$

$$P_{T}\left(x_{1},x_{1}\right)=\frac {e}{e+2}\frac {1}{e+1}=\frac {e}{(e+2)(e+1)}$$

In conclusion, we construct physical models in the Fock space over the Hilbert space of tokens realizing large language models based on a transformer archi-tecture as open quantum systems. Note that physical models satisfying the joint probability distributions as-sociated with a transformer TransfL are not necessar-ily unique. However, a physical model{E(\ell,0)}\ell=1uniquely determines the joint probability distributions, that is, it defines a unique physical process for operat-ing the large language model based on TransfsfL.There-for,in a physical model{E(t\ell,t0)}\ell=1Lfor TransfsfL,train-ing for TransfL corresponds to training for the quan-tum operations{E(t\ell,t0)}\ell=1L,which are adjustable and learned during the training process, determining the physical model, as corresponding to the parameter ma-trixes{(W\ellQW\ellKW\ellV)}\ell=1L in TransfL. From a physical perspective, training for a large language model is just to determine the quantum operations{E(t\ellt0)}\ell=1L as-sociated with the corresponding open quantum system (cf.[10]). This means that our physical models underlie the transformer architecture for large language models. We refer to [3] for a mathematical foundation of general AI, including quantum AI.

**Appendix.**In this appendix, we show that if a building block consists of an attention mechanism (WQ,WK,WV)and FFN in a transformer architecture, then there is a quantum operation 3 in H depending on-lyo(WQ,WK,WV)and FFN such that given an input textT={xi}i=1nfor the input state

$$ρ_{T}=\text {diag}\left(0,0^{(1)},\cdots ,0^{(n-1)},\left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \cdots \otimes \left|x_{n}\right\rangle \left\langle x_{n}\right|,0^{(n+1)},\cdots \right)\in \mathcal {S}(\mathbb {H})$$

the quantum operation 3satisfies

$$\mathcal {E}ρ_{T}=\sum _{i=1}^{n}\text {softmax}\left(S^{(n)}\right)_{i}\text {diag}\left(0,0^{(1)}\cdots ,0^{(n)},\left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \cdots \otimes \left|x_{n}\right\rangle \left\langle x_{n}\right|\otimes \left|y_{i}\right\rangle \left\langle y_{i}\right|,0^{(n+2)},\cdots \right),$$

whereyi=FFN(WVxi)'s(i∈[n])are given by((WQ,WK,WV) and FFN.

To this end, we regard 1,|x〉〈x|and|x1〉〈x1|⊗⋯⊗|x〉〈x| as elements in C(H) in a natural way,i.e.,

$$1\;eq\text {diag}\left(1,0^{(1)},0^{(2)},\cdots \right)$$

$$|x\rangle \langle x|\;eq\text {diag}\left(0,|x\rangle \langle x|,0^{(2)},,\cdots \right)\quad \left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \cdots \otimes \left|x_{n}\right\rangle \left\langle x_{n}\right|\;eq\text {diag}\left(0,0^{(1)},\cdots ,0^{(n-1)},\left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \cdots \otimes \left|x_{n}\right\rangle \left\langle x_{n}\right|,0^{(n+1)},\cdots \right),$$

forn≥1.We first define

$$Φ(1)=\left|x_{0}\right\rangle \left\langle x_{0}\right|$$

wherex0∈Tis a certain element. Secondly,define

$$Φ(|x\rangle \langle x|)=\text {diag}\left(0,0^{(1)},|x\rangle \langle x|\otimes \left|\text {FFN}\left(W^{V}x\right)\right\rangle \left\langle \text {FFN}\left(W^{V}x\right)\right|,0^{(3)},\cdots \right)\quad \quad \forall x\in \mathbf {T},$$

and in general,forn∈[L]define

$$Φ\left(\left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \cdots \otimes \left|x_{n}\right\rangle \left\langle x_{n}\right|\right)\quad =\sum _{i=1}^{n}\text {softmax}\left(S^{(n)}\right)_{i}\text {diag}\left(0,0^{(1)}\cdots ,0^{(n)},\left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \cdots \otimes \left|x_{n}\right\rangle \left\langle x_{n}\right|\otimes \left|y_{i}\right\rangle \left\langle y_{i}\right|,0^{(n+2)},\cdots \right)$$

for anyxi∈T andi∈[n].Let

$$\mathbf {S}=\text {span}\left\{1,\left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \cdots \otimes \left|x_{\ell }\right\rangle \left\langle x_{\ell }\right|:\right.\quad \quad x_{i}\in \mathbf {T},i\in [\ell ];\quad \ell =1,\cdots ,L\}$$

<!-- This version posted 2025-05-27. -->

<!-- <table border="1" ><tr>
<td>I A 9<br>9<br>7<br>0<br>0<br>.<br>S O S 7<br>0<br>Z :</td>
</tr><tr>
<td>A X I e u I O</td>
</tr></table> -->

<!-- 6 -->

Then Φ extends uniquely to a positive map EΦ from S into L(H) ,that is,

$$\mathcal {E}_{Φ}\left(a_{0}+\sum _{x\in \mathbf {T}}a_{x}|x\rangle \langle x|+\cdots +\sum _{x_{1},\cdots ,x_{n}\in \mathbf {T}}a_{x_{1},\cdots ,x_{n}}\left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \cdots \otimes \left|x_{n}\right\rangle \left\langle x_{n}\right|+\cdots \right)\quad =a_{0}\left|x_{0}\right\rangle \left\langle x_{0}\right|+\sum _{x\in \mathbf {T}}a_{x}Φ(|x\rangle \langle x|)+\cdots +\sum _{x_{1},\cdots ,x_{n}\in \mathbf {T}}a_{x_{1},\cdots ,x_{n}}Φ\left(\left|x_{1}\right\rangle \left\langle x_{1}\right|\otimes \cdots \otimes \left|x_{n}\right\rangle \left\langle x_{n}\right|\right)+\cdots ,$$

wherea0ax,ax1,⋯,xn are any complex numbers forn≥1.Note thatS is a commutative C*-algebra. By Stine-spring's theorem (cf.[8,Theorem 3.11]),EΦ:S↦L(H)is completely positive. Hence,by Arveson's extension the-orem (cf.[8,Theorem 7.5]),EΦ extends to a completely positive operator 3 in L(H),i.e., a quantum operation in H (note that 3 is not necessarily unique). By the con-struction,3 satisfies the required condition.

[1] D.Bahdanau, K. Cho,Y. Bengio, Neural machine trans-lation by jointly learning to align and translate, arXiv (2014),1409.0473.

[2]H.P. Breuer, F. Petruccione, The Theory of Open Quan-tum Systems,Oxford University Press, Oxford, 2002.

[3] Z. Chen, L. Ding, H. Liu, J. Yu, A topos-theoretic formalism of quantum artificial intellegence (in Chi-nese), Scientia Sinica Mathematica 55 (2025), online: www.sciengine.com/SSM/doi/10.1360/SSM-2024-0126.

[4] B.Geshkovski,C. Letrouit,Y.Polyyanskiy,P.Rigollet, A mathematical perspective on transformers,Bulletin of the American Mathematical Society,2025,in press.

[5] I. Goodfellow, Y. Bengio, A. Courville, Deep Learning, MIT Press, 2016.

[6] S. Minaee, et al., Large language models: A survey,arXiv (2025),2402.06196v3.

[7] M.A.Nielsen and I.L.Chuang, Quantum computation and quantum information, Cambridge University Press, Cam-bridge,2001.

[8] V. Paulsen, Completely Bounded Maps and Operator Al-gebras, Cambridge University Press, Cambridge, 2002. [9] M. Reed, B. Simon, Method of Mordern Mathematical Physics, Vol. I, Academic Press, San Diego, 1980.

[10] K. Sharma, M.Cerezo, L. Cincio, P.J. Coles, Trainability of dissipative perceptron-based quantum neural network-s, Physical Review Letters **128** (2022),180505:1-7.

[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,L. Jones, A.N Gomez, L. Kaiser, I. Polosukhin, Attention is all you need, Advances in Neural Information Processing Systems, 30 (2017),5998-6008.

[12] C.J. Villas-Boas, C.E. Máximo, P.J. Paulino, R.P. Bachelard, G. Rempe, Bright and dark states of light: The quantum origin of classical interference,Physical Re-view Letters 134(2025),133603:1-6.

[13] J.Vuckovic, A. Baratin, R.T. Combes, A mathematical theory of attention, arXiv (2020),2007.02876.

[14] Y.Zhang, et al., Tensor product attention is all you need, arXiv (2025),2501.06425.

[15] W.X. Zhao, et al., A survey of large language models, arXiv (2025),2303.18223v16.

<!-- This version posted 2025-05-27. -->

<!-- <table border="1" ><tr>
<td></td>
</tr><tr>
<td>I A 9<br>9<br>7<br>0<br>0<br>.<br>S O S 7<br>0<br>Z :</td>
</tr><tr>
<td>A I X e u I O</td>
</tr></table> -->


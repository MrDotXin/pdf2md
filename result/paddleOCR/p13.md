ChinaXiv:202505.00266v1
# Physical models realizing the transformer architecture of large language models
Zeqian Chen $ ^{*} $ 
Wuhan Institute of Physics and Mathematics, IAPM, Chinese Academy of Sciences, West District 30, Xiao-Hong-Shan, Wuhan 430071, China
The introduction of the transformer architecture in 2017 (cf. $ [11] $ ) marked the most striking advancement in natural language processing. The transformer is a model architecture relying entirely on an attention mechanism to draw global dependencies between input and output. However, we believe there is a gap in our theoretical understanding of what the transformer is, and why it works physically. In this paper, from a physical perspective on modern chips, we construct physical models in the Fock space over the Hilbert space of tokens realizing large language models based on a transformer architecture as open quantum systems. Our physical models underlie the transformer architecture for large language models.
## PACS numbers:
Large language models (LLMs for short) are based on deep neural networks (DNNs) (cf. $ [5] $ ), and a common characteristic of DNNs is their compositional nature: data is processed sequentially, layer by layer, resulting in a discrete-time dynamical system. The introduction of the transformer architecture in 2017 marked the most striking advancement in terms of DNNs (cf. $ [11] $ ). Indeed, the transformer is a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. At each step, the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next. The transformer has achieved great success in natural language processing (cf. $ [15] $  and references therein).
The transformer has a modularization framework and is constructed by two main building blocks: self-attention and feed-forward neutral networks. Self-attention is an attention mechanism (cf. $ [1] $ ) relating different positions of a single sequence in order to compute a representation of the sequence. In line with successful large language models, one often focuses on the decoder-only setting of the transformer, where the model iteratively predicts the next tokens based on a given sequence of tokens. This procedure is coined autoregressive since the prediction of new tokens is only based on previous tokens. Such conditional sequence generation using autoregressive transformers is referred to as the transformer architecture. However, despite its meteoric rise within deep learning, we believe there is a gap in our theoretical understanding of what the transformer is, and why it works physically (cf. $ [6] $ ).
To the best of our knowledge, physical models for the transformer architecture of large language models are usually described by using systems of mean-field interacting particles (cf. [4, 13] and references therein), i.e., large language models are regarded as classical statistical systems. However, since modern chips process data through controlling the flow of electric current, i.e., the dynamics of largely many electrons, so they should be regarded as quantum statistical ensembles and open quantum systems from a physical perspective (cf.[2, 12]). In this paper, we construct physical models in the Fock space over the Hilbert space of tokens realizing large language models based on a transformer architecture as open quantum systems.
In the transformer architecture of a large language model S, we assume the finite set T of tokens in S has been embedded in  $ R^{d} $ , where d is called the embedding dimension, so we identify each  $ t \in T $  with one of finitely-many vectors x in  $ R^{d} $ . We assume that the structure (positional information, adjacency information, etc) is encoded in these vectors. A finite sequence  $ \{x_{i}\}_{i=1}^{n} $  of tokens is called a text for S, simply denoted by  $ T = x_{1}x_{2} \cdots x_{n} $  or  $ (x_{1}, x_{2}, \cdots, x_{n}) $ , where n is called the length of the text T. We write  $ [n] = \{1, 2, \ldots, n\} $  for an integer n. For a Hilbert space K, we use  $ \mathcal{L}(\mathbb{K}) $  and  $ \mathcal{S}(\mathbb{K}) $  respectively to denote the set of all linear bounded operators and the set of all density operators in K.
Recall that a self-attention layer SelfAtt with an attention mechanism  $ (W^{Q}, W^{K}, W^{V}) $  in the transformer architecture is the only layer that combines different tokens, where  $ W^{Q} $  and  $ W^{K} $  are two  $ d' \times d $  real matrices (i.e., the query and key matrices) and  $ W^{V} $  is the  $ d \times d $  real matrix (called the value matrix) such that  $ W^{V}x \in T $  for  $ x \in T $ . Let us denote the input text to the layer by  $ X = \{x_{i}\}_{i=1}^{n} $ . For each  $ i \in [n] $ , letting
 $$ s_{i}=\frac{1}{\sqrt{d}}\langle W^{Q}x_{n},W^{K}x_{i}\rangle,\quad\forall i\in[n], $$ 
we can interpret  $  S^{(n)} = \{ s_{i} \}_{i=1}^{n}  $  as similarities between the n-th token  $  x_{n}  $  (i.e., the query) and the other tokens (i.e., keys). The softmax layer is given by
 $$  softmax(S^{(n)})_{i}=\frac{e^{s_{i}}}{\sum_{j=1}^{n}e^{s_{j}}},\quad\forall i\in[n], $$ 
which can be interpreted as the probability for the n-th query to “attend” to the i-th key. Then the self-attention
 $ ^{*} $ Electronic address: chenzeqian@hotmail.com
This version posted 2025-05-27.
ChinaXiv:202505.00266v1
2
layer SelfAtt is defined by
 $$ \mathrm{S e l f A t t}(X)_{n}=\sum_{i=1}^{n}\mathrm{s o f t m a x}(S^{(n)})_{i}W^{V}x_{i}, $$ 
(1)
indicating that the output  $ W^{V}x_{i} $  occurs with the probability  $ \text{softmax}(S^{(n)})_{i} $ , which is often referred to as the values of the token  $ x_{i} $ . Note that  $ (W^{Q}, W^{K}, W^{V}) $  are trainable parameters in the transformer architecture.
In the same block, a feed-forward neural network FFN is then applied to  $ W^{V}x_{i} $ 's such that  $ y_{i}=\mathrm{FFN}(W^{V}x_{i}) $  with the probability  $ \mathrm{softmax}(S^{(n)})_{i} $  for each  $ i\in[n] $ , and so the output is  $ x_{n+1}=y_{i}=\mathrm{FFN}\circ\mathrm{SelfAtt}(\{x_{i}\}_{i=1}^{n}) $  for some  $ i\in[n] $ . One can then apply the same operations to the extended sequence  $ x_{1}x_{2}\cdots x_{n}x_{n+1} $  in a next block, obtaining  $ x_{n+2}=\mathrm{FFN}^{\prime}\circ\mathrm{SelfAtt}^{\prime}(\{x_{i}\}_{i=1}^{n+1}) $ , to iteratively compute further tokens (there is usually a stopping criterion based on a special token).
Typically, a transformer of depth L is defined by a composition of L blocks, denoted by Transf $ _{L} $ , consisting of L self-attention maps  $ \{SelfAtt_{\ell}\}_{\ell=1}^{L} $  and L feed-forward neural networks  $ \{FFN_{\ell}\}_{\ell=1}^{L} $ , i.e.,
 $$  Transf_{L}=(FFN_{L}\circ SelfAtt_{L})\circ\cdots\circ(FFN_{1}\circ SelfAtt_{1}) $$ 
(2)
where the indices of the layers SelfAtt and FFN in (2) indicate the use of different trainable parameters in each of the block. Then, given an input text  $ T = x_{1} \cdots x_{n} $ , Transf $ _{L} $  generates a text  $ y_{i_{1}} \cdots y_{i_{L}} $  with the joint probability
 $$ P_{T}(y_{i_{1}},\cdots,y_{i_{L}})=softmax(S_{1}^{(n)})_{i_{1}}\cdots softmax(S_{L}^{(n+L-1)})_{i_{L}} $$ 
where softmax$(S^{(n+\ell-1)\ell})_{i\ell}$ is given by the attention mechanism $(W_{\ell}^{Q}, W_{\ell}^{K}, W_{\ell}^{V})$ in the $\ell$-th building block for each $\ell=1,\ldots,L$.
Now, we are ready to construct a physical model realizing the transformer architecture (2) for LLMs. To this end, consider a large language model S with the set T of N tokens embedded in  $ R^{d} $ . Let h be the Hilbert space with  $ \{|x\rangle: x \in T\} $  being an orthogonal basis, and we identify  $ x = |x\rangle $  for  $ x \in T $ . Let  $ \mathbb{H} = \mathcal{F}(\mathbf{h}) $  be the Fock space over h, that is,
 $$ \mathcal{F}(\mathbf{h})=\mathbb{C}\oplus\bigoplus_{n=1}^{\infty}\mathbf{h}^{\otimes n}, $$ 
where  $ h^{\otimes n} $  is the n-fold tensor product of h (cf. $ ^{[9]} $ ) In what follows, for the sake of convenience, we involve the finite Fock space
 $$ \mathbb{H}=\mathcal{F}^{(M)}(\mathbf{h})=\mathbb{C}\oplus\bigoplus_{n=1}^{M}\mathbf{h}^{\otimes n} $$ 
where M is an integer such that  $ M \gg N $ . Note that an operator  $ A^{(n)} = A_{1} \otimes \cdots \otimes A_{n} \in \mathcal{L}(\mathbf{h}^{\otimes n}) $  for  $ A_{j} \in \mathcal{L}(\mathbf{h}) $  satisfies that for all  $ h^{(n)} = h_{1} \otimes \cdots \otimes h_{n} \in \mathbf{h}^{\otimes n} $ ,
 $$ A h^{(n)}=\left(A_{1}h_{1}\right)\otimes\cdots\otimes\left(A_{n}h_{n}\right)\in\mathbf{h}^{\otimes n}, $$ 
and in particular, if $\rho_{i}\in\mathcal{S}(\mathbf{h})$ for $i\in[n]$, then $\rho^{(n)}=\rho_{1}\otimes\cdots\otimes\rho_{n}\in\mathcal{S}(\mathbf{h}^{\otimes n})$. Given $\alpha\in\mathbb{C}$ and a sequence $A^{(n)}\in\mathcal{L}(\mathbf{h}^{\otimes n})$ for $n\in[M]$, the operator $\mathrm{diag}(\alpha,A^{(1)},\cdots,A^{(M)})\in\mathcal{L}(\mathbb{H})$ is defined by
 $$ \mathrm{diag}(\alpha,A^{(1)},\cdots,A^{(M)})\mathrm{h}^{(M)}=(\alpha c,A^{(1)}h^{(1)},\cdots,A^{(M)}h^{(M)}) $$ 
for every  $ \mathbf{h}^{(M)}=(c,h^{(1)},\cdots,h^{(M)})\in\mathbb{H} $ . In particular, if  $ \rho^{(n)}\in\mathcal{S}(\mathbf{h}^{\otimes n}) $ , then
 $$ \rho^{(M)}=\mathrm{diag}(0,0^{(1)},\cdots,0^{(n-1)},\rho^{(n)},0^{(n+1)},\cdots,0^{(M)})\in\mathcal{S}(\mathbb{H}), $$ 
where  $ 0^{(i)} $  denotes the zero operator in  $ h^{\otimes i} $  for  $ i \geq 1 $ .
It suffices to construct a physical model in the Fock space  $ \mathbb{H}=\mathcal{F}^{(M)}(\mathbf{h})\;(\mathbf{M}\gg L) $  for a transformer Transf $ _{L} $  (2) with a composition of L blocks, consisting of L self-attention maps  $ \{SelfAtt_{\ell}=(W_{\ell}^{Q},W_{\ell}^{K},W_{\ell}^{V})\}_{\ell=1}^{L} $  and L feed-forward neural networks  $ \{FFN_{\ell}\}_{\ell=1}^{L} $ . Precisely, let us denote the input text to the layer by  $ T=\{x_{i}\}_{i=1}^{n} $ . As noted above, for  $ \ell=1,\ldots,L $ , one has
 $$ \mathrm{F F N}_{\ell}\circ\mathrm{S e l f A t t}_{\ell}(T)=\sum_{i=1}^{n+\ell-1}\mathrm{s o f t m a x}(S_{\ell}^{(n+\ell-1)})_{i}\mathrm{F F N}_{\ell}(W^{V_{\ell}}x_{i}), $$ 
where  $ S_{\ell}^{(n+\ell-1)}=\{s_{i}^{(\ell)}\}_{i=1}^{n+\ell-1} $  and
 $$ s_{i}^{(\ell)}=\frac{1}{\sqrt{d}}\langle W^{Q_{\ell}}x_{n+\ell-1},W^{K_{\ell}}x_{i}\rangle,\quad\forall i\in[n+\ell-1]. $$ 
A physical model needed to construct for Transf $ _{L} $  consist of an input  $ \rho(t_{0}) $  and a sequence of quantum operations  $ \{\mathcal{E}(t_{\ell},t_{0})\}_{\ell=1}^{L} $  in the Fock space H (cf.[7]), where  $ t_{0} < t_{1} < \cdots < t_{L} $ . We show how to construct this model step by step as follows.
To this end, we denote by $\Omega=\{\diamond\}\cup\mathbf{T}$ and by $2^{\Omega}$ the
This version posted 2025-05-27.
ChinaXiv:202505.00266v1
3
set of all subsets of $\Omega$, and write $\mathbf{D}=(\{\omega\}:\omega\in\Omega)$. At first, for an input text $T=x_{1}\cdots x_{n}$, the input state $\rho_{T}$ is given as
 $$ \rho_{T}=\rho(t_{0})=\mathrm{diag}(0,0^{(1)},\cdots,0^{(n-1)},|x_{1}\rangle\langle x_{1}|\otimes\cdots\otimes|x_{n}\rangle\langle x_{n}|,0^{(n+1)},\cdots)\in\mathcal{S}(\mathbb{H}). $$ 
Then there is a quantum operation  $ \mathcal{E}(t_{1},t_{0}) $  in H depending only on the attention mechanism  $ (W_{1}^{Q},W_{1}^{K},W_{1}^{V}) $  and  $ FFN_{1} $  such that (see Appendix for the details)
 $$ \begin{align*}\mathcal{E}(t_{1},t_{0})\rho(t_{0})\\=\sum_{i=1}^{n}\mathrm{softmax}(S_{1}^{(n)})_{i}\mathrm{diag}(0,0^{(1)}\cdots,0^{(n)},|x_{1}\rangle\langle x_{1}|\otimes\cdots\otimes|x_{n}\rangle\langle x_{n}|\otimes|y_{i}^{(1)}\rangle\langle y_{i}^{(1)}|,0^{(n+2)},\cdots),\end{align*} $$ 
(3)
where  $ y_{i}^{(1)}=\mathrm{FFN}_{1}(W^{V_{1}}x_{i}) $  and  $ \{y_{i}^{(1)}\}_{i=1}^{n}\subset\{|x\rangle:x\inT\} $ . Define  $ X_{1}:2^{\Omega}\mapsto\mathcal{E}(\mathbb{H}) $  by
 $$ X_{1}(\{\diamond\})=\mathrm{diag}(1,I_{\mathbf{h}},\cdots,I_{\mathbf{h}}^{\otimes n},0^{(n+1)},I_{\mathbf{h}^{\otimes(n+2)}},\cdots), $$ 
and for every  $ x \in T $ ,
 $$ X_{1}(\{x\})=\mathrm{diag}(0,0^{(1)},\cdots,0^{(n)},\underbrace{I_{\mathbf{h}}\otimes\cdots\otimes I_{\mathbf{h}}}_{n}\otimes|x\rangle\langle x|,0^{(n+2)},\cdots). $$ 
Making a measurement  $ (X_{1}, \mathbf{D}) $  at time  $ t_{1} $ , we obtain an output  $ y_{i}^{(1)} $  with probability  $ \operatorname{softmax}(S_{1}^{(n)})_{i} $  and, according to the von Neumann-Lüders reduction postulate (cf. $ ^{[7]} $ ), the appropriate density operator to use for any further calculation is
 $$ \begin{align*}\rho_{\mathrm{red}}(t_{1})_{i}=&\frac{E_{i}^{(1)}\rho(t_{1})E_{i}^{(1)}}{\mathrm{Tr}[E_{i}^{(1)}\rho(t_{1})]}\\=&\mathrm{diag}(0,0^{(1)}\cdots,0^{(n)},|x_{1}\rangle\langle x_{1}|\otimes\cdots\otimes|x_{n}\rangle\langle x_{n}|\otimes|y_{i}^{(1)}\rangle\langle y_{i}^{(1)}|,0^{(n+2)},\cdots),\end{align*} $$ 
for every $i\in[n]$, where $\rho(t_{1})=\mathcal{E}(t_{1},t_{0})\rho(t_{0})$, and
 $$ E_{i}^{(1)}=\mathrm{diag}(0,0^{(1)},\cdots,0^{(n)},\underbrace{I_{\mathbf{h}}\otimes\cdots\otimes I_{\mathbf{h}}}_{n}\otimes|y_{i}^{(1)}\rangle\langle y_{i}^{(1)}|,0^{(n+2)},\cdots). $$ 
Next, there is a quantum operation  $ \mathcal{E}(t_{2}, t_{0}) $  in H depending only on the attention mechanism  $ (W_{2}^{Q}, W_{2}^{K}, W_{2}^{V}) $  and  $ FFN_{2} $  at time  $ t_{2} $  such that (see Appendix again)
 $$ \begin{aligned}&\mathcal{E}(t_{2},t_{0})\rho_{\mathrm{red}}(t_{1})_{i_{1}}=\sum_{i_{2}=1}^{n+1}\operatorname{softmax}(S_{2}^{(n+1)})_{i_{2}}\\ &\quad\times\mathrm{diag}(0,0^{(1)},\cdots,0^{(n+1)},|x_{1}\rangle\langle x_{1}|\otimes\cdots\otimes|x_{n}\rangle\langle x_{n}|\otimes|y_{i_{1}}^{(1)}\rangle\langle y_{i_{1}}^{(1)}|\otimes|y_{i_{2}}^{(2)}\rangle\langle y_{i_{2}}^{(2)}|,0^{(n+3)},\cdots)\\ \end{aligned} $$ 
(4)
for  $ i_{1}\in[n] $ , where  $ y_{i_{2}}^{(2)}=\mathrm{FFN}_{2}(W^{V_{2}}x_{i_{2}}) $  (with  $ x_{n+1}=y_{i_{1}}^{(1)} $ ) and  $ \{y_{i}^{(2)}\}_{i=1}^{n+1}\subset\{|x\rangle:x\inT\} $ . Define  $ X_{2}:2^{\Omega}\mapsto\mathcal{E}(\mathbb{H}) $  by
 $$ X_{2}(\{\diamond\})=\mathrm{diag}(1,I_{\mathbf{h}},\cdots,I_{\mathbf{h}}^{\otimes(n+1)},0^{(n+2)},I_{\mathbf{h}^{\otimes(n+3)}},\cdots), $$ 
and for every  $ x \in T $ ,
 $$ X_{2}(\{x\})=\mathrm{diag}(0,0^{(1)},\cdots,0^{(n+1)},\underbrace{I_{\mathbf{h}}\otimes\cdots\otimes I_{\mathbf{h}}}_{n+1}\otimes|x\rangle\langle x|,0^{(n+3)},\cdots). $$ 
Making a measurement  $ (X_{2}, \mathbf{D}) $  at time  $ t_{2} $ , we obtain an output  $ y_{i_{2}}^{(2)} $  with probability  $ \text{softmax}(S_{2}^{(n+1)})_{i_{2}} $  and the appropriate density operator to use for any further calculation is
 $$ \begin{align*}\rho_{\mathrm{red}}(t_{2})_{i_{1},i_{2}}=&\frac{E_{i_{2}}^{(2)}\rho(t_{2})_{i_{1}}E_{i_{2}}^{(2)}}{\mathrm{Tr}[E_{i_{2}}^{(2)}\rho(t_{2})_{i_{1}}]}\\=&\mathrm{diag}(0,0^{(1)},\cdots,0^{(n+1)},|x_{1}\rangle\langle x_{1}|\otimes\cdots\otimes|x_{n}\rangle\langle x_{n}|\otimes|y_{i_{1}}^{(1)}\rangle\langle y_{i_{1}}^{(1)}|\otimes|y_{i_{2}}^{(2)}\rangle\langle y_{i_{2}}^{(2)}|,0^{(n+3)},\cdots),\end{align*} $$ 
This version posted 2025-05-27.
4
ChinaXiv:202505.00266v1
for each  $ i_{2} \in [n+1] $ , where  $ \rho(t_{2})_{i_{1}} = \mathcal{E}(t_{2}, t_{0}) \rho_{\mathrm{red}}(t_{1})_{i_{1}} $  and
 $$ E_{i_{2}}^{(2)}=\mathrm{diag}(0,0^{(1)},\cdots,0^{(n+1)},\underbrace{I_{\mathbf{h}}\otimes\cdots\otimes I_{\mathbf{h}}}_{n+1}\otimes|y_{i_{2}}^{(2)}\rangle\langle y_{i_{2}}^{(2)}|,0^{(n+3)},\cdots). $$ 
Step by step, we obtain a physical model  $ \{\mathcal{E}(t_{\ell},t_{0})\}_{\ell=1}^{L} $  with the input state  $ \rho_{T}=\rho(t_{0}) $  as given an input text  $ T=x_{1}\cdots x_{n} $ , such that a text  $ (y_{i_{1}}^{(1)},y_{i_{2}}^{(2)},\ldots,y_{i_{L}}^{(L)}) $  is generated with the probability
 $$ \begin{aligned}\mathrm{P}_{T}(y_{i_{1}}^{(1)},y_{i_{2}}^{(2)},\cdots,y_{i_{L}}^{(L)})\\ =&\mathrm{softmax}(S_{1}^{(n)})_{i_{1}}\cdots\mathrm{softmax}(S_{L}^{(n+L-1)})_{i_{L}},\end{aligned} $$ 
within the sequential measurement  $ (X_{1}, \mathbf{D}), \ldots, (X_{L}, \mathbf{D}) $ . Thus, the physical model so constructed realizes the transformer architecture  $ Transf_{L} $ .

A physical model for the transformer with a multi-headed attention (cf. $ [11] $ ) can be constructed in a similar way. Also, we can construct physical models for the transformer of more complex structure (cf. $ [14] $  and reference therein). We omit the details.
Example. Let  $ T=\{x_{0},x_{1}\} $  be the set of two tokens embedded in  $ R^{2} $  such that  $ x_{0}=(1,0) $  and  $ x_{1}=(0,1) $ . Then  $ h=C^{2} $  with the standard basis  $ |0\rangle=|x_{0}\rangle $  and  $ |1\rangle=|x_{1}\rangle $ . Let  $ \mathbb{H}=\mathcal{F}^{(6)}(\mathbb{C}^{2}) $ . Assume an input text  $ T=(x_{0},x_{1},x_{0}) $ . The input state  $ \rho_{T} $  is then given by
 $$ \rho_{T}=\rho(t_{0})=\mathrm{diag}(0,0^{(1)},0^{(2)},|x_{0}\rangle\langle x_{0}|\otimes|x_{1}\rangle\langle x_{1}|\otimes|x_{0}\rangle\langle x_{0}|,0^{(4)},0^{(5)},0^{(6)}). $$ 
If  $ W_{1}^{Q} = W_{1}^{V} = FFN_{1} = I $  and  $ W_{1}^{K} = \sigma_{x} $  in  $ R^{2} $ , an associated physical operation  $ \mathcal{E}(t_{1}, t_{0}) $  at time  $ t_{1} $  satisfies
 $$ \begin{align*}\mathcal{E}(t_{1},t_{0})\rho(t_{0})=&\frac{2}{e+2}\mathrm{diag}(0,0^{(1)},0^{(2)},0^{(3)},|x_{0}\rangle\langle x_{0}|\otimes|x_{1}\rangle\langle x_{1}|\otimes|x_{0}\rangle\langle x_{0}|\otimes|x_{0}\rangle\langle x_{0}|,0^{(5)},0^{(6)})\\&+\frac{e}{e+2}\mathrm{diag}(0,0^{(1)},0^{(2)},0^{(3)},|x_{0}\rangle\langle x_{0}|\otimes|x_{1}\rangle\langle x_{1}|\otimes|x_{0}\rangle\langle x_{0}|\otimes|x_{1}\rangle\langle x_{1}|,0^{(5)},0^{(6)}).\end{align*} $$ 
By measurement, we obtain  $ x_{0} $  with probability  $ \frac{2}{e+2} $  and obtain  $ x_{1} $  with probability  $ \frac{e}{e+2} $ , while
 $$ \begin{align*}\rho_{\mathrm{red}}(t_{1})_{0}=&\mathrm{diag}(0,0^{(1)},0^{(2)},0^{(3)},|x_{0}\rangle\langle x_{0}|\otimes|x_{1}\rangle\langle x_{1}|\otimes|x_{0}\rangle\langle x_{0}|\otimes|x_{0}\rangle\langle x_{0}|,0^{(5)},0^{(6)}),\\\rho_{\mathrm{red}}(t_{1})_{1}=&\mathrm{diag}(0,0^{(1)},0^{(2)},0^{(3)},|x_{0}\rangle\langle x_{0}|\otimes|x_{1}\rangle\langle x_{1}|\otimes|x_{0}\rangle\langle x_{0}|\otimes|x_{1}\rangle\langle x_{1}|,0^{(5)},0^{(6)}).\end{align*} $$ 
If W2Q = W2K = FFN2 = I and W2V = σx in R2, an associated quantum operation E(t2, t0) at time t2 satisfies
 $$ \begin{align*}\mathcal{E}(t_{2},t_{0})&\rho_{\mathrm{red}}(t_{1})_{0}\\=&\frac{1}{3e+1}(0,0^{(1)},0^{(2)},0^{(3)},0^{(4)},|x_{0}\rangle\langle x_{0}|\otimes|x_{1}\rangle\langle x_{1}|\otimes|x_{0}\rangle\langle x_{0}|\otimes|x_{0}\rangle\langle x_{0}|\otimes|x_{0}\rangle\langle x_{0}|,0^{(6)})\\&+\frac{3e}{3e+1}(0,0^{(1)},0^{(2)},0^{(3)},0^{(4)},|x_{0}\rangle\langle x_{0}|\otimes|x_{1}\rangle\langle x_{1}|\otimes|x_{0}\rangle\langle x_{0}|\otimes|x_{0}\rangle\langle x_{0}|\otimes|x_{1}\rangle\langle x_{1}|,0^{(6)}).\end{align*} $$ 
and
 $$ \begin{align*}&\mathcal{E}(t_{2},t_{0})\rho_{\mathrm{red}}(t_{1})_{1}\\=&\frac{e}{e+1}(0,0^{(1)},0^{(2)},0^{(3)},0^{(4)},|x_{0}\rangle\langle x_{0}|\otimes|x_{1}\rangle\langle x_{1}|\otimes|x_{0}\rangle\langle x_{0}|\otimes|x_{1}\rangle\langle x_{1}|\otimes|x_{0}\rangle\langle x_{0}|,0^{(6)})\\&+\frac{1}{e+1}(0,0^{(1)},0^{(2)},0^{(3)},0^{(4)},|x_{0}\rangle\langle x_{0}|\otimes|x_{1}\rangle\langle x_{1}|\otimes|x_{0}\rangle\langle x_{0}|\otimes|x_{1}\rangle\langle x_{1}|\otimes|x_{1}\rangle\langle x_{1}|,0^{(6)}).\end{align*} $$ 
By measurement at time  $ t_{2} $ , when  $ x_{0} $  occurs at  $ t_{1} $ , we obtain  $ x_{0} $  with probability  $ \frac{1}{3e+1} $  and obtain  $ x_{1} $  with probability  $ \frac{3e}{3e+1} $ ; when  $ x_{1} $  occurs at  $ t_{1} $ , we obtain  $ x_{0} $  with probability  $ \frac{e}{e+1} $  and obtain  $ x_{1} $  with probability  $ \frac{1}{e+1} $ .
This version posted 2025-05-27.
ChinaXiv:202505.00266v1
5
Hence, we obtain the joint probability distributions:
 $$ \begin{aligned}&P_{T}(x_{0},x_{0})=\frac{2}{e+2}\frac{1}{3e+1}=\frac{2}{(e+2)(3e+1)},\\&P_{T}(x_{0},x_{1})=\frac{2}{e+2}\frac{3e}{3e+1}=\frac{6e}{(e+2)(3e+1)},\\&P_{T}(x_{1},x_{0})=\frac{e}{e+2}\frac{e}{e+1}=\frac{e^{2}}{(e+2)(e+1)},\\&P_{T}(x_{1},x_{1})=\frac{e}{e+2}\frac{1}{e+1}=\frac{e}{(e+2)(e+1)}.\\ \end{aligned} $$ 
In conclusion, we construct physical models in the Fock space over the Hilbert space of tokens realizing large language models based on a transformer architecture as open quantum systems. Note that physical models satisfying the joint probability distributions associated with a transformer  $ Transf_{L} $  are not necessarily unique. However, a physical model  $ \{\mathcal{E}(t_{\ell},t_{0})\}_{\ell=1}^{L} $  uniquely determines the joint probability distributions, that is, it defines a unique physical process for operating the large language model based on  $ Transf_{L} $ . Therefore, in a physical model  $ \{\mathcal{E}(t_{\ell},t_{0})\}_{\ell=1}^{L} $  for  $ Transf_{L} $ , training for  $ Transf_{L} $  corresponds to training for the quantum operations  $ \{\mathcal{E}(t_{\ell},t_{0})\}_{\ell=1}^{L} $ , which are adjustable and learned during the training process, determining the physical model, as corresponding to the parameter matrices  $ \{(W_{\ell}^{Q},W_{\ell}^{K},W_{\ell}^{V})\}_{\ell=1}^{L} $  in  $ Transf_{L} $ . From a physical perspective, training for a large language model is just to determine the quantum operations  $ \{\mathcal{E}(t_{\ell},t_{0})\}_{\ell=1}^{L} $  associated with the corresponding open quantum system (cf. $ ^{[10]} $ ). This means that our physical models underlie the transformer architecture for large language models. We refer to [3] for a mathematical foundation of general AI, including quantum AI.
Appendix. In this appendix, we show that if a building block consists of an attention mechanism $(W^{Q}, W^{K}, W^{V})$ and FFN in a transformer architecture, then there is a quantum operation $\mathcal{E}$ in H depending only on $(W^{Q}, W^{K}, W^{V})$ and FFN such that given an input text $T = \{x_{i}\}_{i=1}^{n}$, for the input state
 $$ \rho_{T}=\mathrm{diag}(0,0^{(1)},\cdots,0^{(n-1)},|x_{1}\rangle\langle x_{1}|\otimes\cdots\otimes|x_{n}\rangle\langle x_{n}|,0^{(n+1)},\cdots)\in\mathcal{S}(\mathbb{H}), $$ 
the quantum operation E satisfies
 $$ \mathcal{E}\rho_{T}=\sum_{i=1}^{n}\mathrm{s o f t m a x}(S^{(n)})_{i}\mathrm{d i a g}(0,0^{(1)}\cdots,0^{(n)},|x_{1}\rangle\langle x_{1}|\otimes\cdots\otimes|x_{n}\rangle\langle x_{n}|\otimes|y_{i}\rangle\langle y_{i}|,0^{(n+2)},\cdots), $$ 
where yi = FFN(WVxi)'s (i ∈ [n]) are given by (WQ, WK, WV) and FFN.
To this end, we regard 1, |x⟩⟨x|, and |x₁⟩⟨x₁| ⊗ . . . ⊗ |xₙ⟩⟨xₙ| as elements in L(H) in a natural way, i.e.,
 $$ \begin{align*}1&\simeq\mathrm{diag}(1,0^{(1)},0^{(2)},\cdots),\\|x\rangle\langle x|&\simeq\mathrm{diag}(0,|x\rangle\langle x|,0^{(2)},,\cdots),\\|x_{1}\rangle\langle x_{1}|&\otimes\cdots\otimes|x_{n}\rangle\langle x_{n}|\simeq\mathrm{diag}(0,0^{(1)},\cdots,0^{(n-1)},|x_{1}\rangle\langle x_{1}|\otimes\cdots\otimes|x_{n}\rangle\langle x_{n}|,0^{(n+1)},\cdots),\end{align*} $$ 
for  $ n \geq 1 $ . We first define
 $$ \Phi(1)=\left|x_{0}\right\rangle\left\langle x_{0}\right| $$ 
where  $ x_{0} \in T $  is a certain element. Secondly, define
 $$ \Phi(|x\rangle\langle x|)=\mathrm{diag}(0,0^{(1)},|x\rangle\langle x|\otimes|\mathrm{FFN}(W^{V} x)\rangle\langle\mathrm{FFN}(W^{V} x)|,0^{(3)},\cdots),\quad\forall x\in\mathbf{T}, $$ 
and in general, for  $ n \in [L] $  define
 $$ \begin{align*}\Phi(|x_{1}\rangle\langle x_{1}|\otimes\cdots\otimes|x_{n}\rangle\langle x_{n}|)\\=\sum_{i=1}^{n}\mathrm{softmax}(S^{(n)})_{i}\mathrm{diag}(0,0^{(1)}\cdots,0^{(n)},|x_{1}\rangle\langle x_{1}|\otimes\cdots\otimes|x_{n}\rangle\langle x_{n}|\otimes|y_{i}\rangle\langle y_{i}|,0^{(n+2)},\cdots)\end{align*} $$ 
for any  $ x_{i} \in T $  and  $ i \in [n] $ . Let
 $$ \mathbf{S}=\operatorname{span}\{1,|x_{1}\rangle\langle x_{1}|\otimes\cdots\otimes|x_{\ell}\rangle\langle x_{\ell}|:x_{i}\in\mathbf{T},i\in[\ell];\ell=1,\cdots,L\}. $$ 
This version posted 2025-05-27.
ChinaXiv:202505.00266v1
6
Then  $ \Phi $  extends uniquely to a positive map  $ E_{\Phi} $  from S into  $ \mathcal{L}(\mathbb{H}) $ , that is,
 $$ \begin{aligned}&\mathcal{E}_{\Phi}\Big(a_{0}+\sum_{x\in\mathbf{T}}a_{x}|x\rangle\langle x|+\cdots+\sum_{x_{1},\cdots,x_{n}\in\mathbf{T}}a_{x_{1},\cdots,x_{n}}|x_{1}\rangle\langle x_{1}|\otimes\cdots\otimes|x_{n}\rangle\langle x_{n}|+\cdots\Big)\\ &=a_{0}|x_{0}\rangle\langle x_{0}|+\sum_{x\in\mathbf{T}}a_{x}\Phi(|x\rangle\langle x|)+\cdots+\sum_{x_{1},\cdots,x_{n}\in\mathbf{T}}a_{x_{1},\cdots,x_{n}}\Phi(|x_{1}\rangle\langle x_{1}|\otimes\cdots\otimes|x_{n}\rangle\langle x_{n}|)+\cdots,\\ \end{aligned} $$ 
where  $ a_{0}, a_{x}, a_{x_{1}, \cdots, x_{n}} $  are any complex numbers for  $ n \geq 1 $ . Note that S is a commutative  $ C^{*} $ -algebra. By Stine-spring's theorem (cf.[8, Theorem 3.11]),  $ \mathcal{E}_{\Phi} : \mathbf{S} \mapsto \mathcal{L}(\mathbb{H}) $  is completely positive. Hence, by Arveson's extension the
[1] D. Bahdanau, K. Cho, Y. Bengio, Neural machine translation by jointly learning to align and translate, arXiv (2014), 1409.0473.
[2] H.P. Breuer, F. Petruccione, The Theory of Open Quantum Systems, Oxford University Press, Oxford, 2002.
[3] Z. Chen, L. Ding, H. Liu, J. Yu, A topos-theoretic formalism of quantum artificial intelligence (in Chinese), Scientia Sinica Mathematica 55 (2025), online: www.sciengine.com/SSM/doi/10.1360/SSM-2024-0126.
[4] B. Geshkovski, C. Letrout, Y. Polyyanskiy, P. Rigollet, A mathematical perspective on transformers, Bulletin of the American Mathematical Society, 2025, in press.
[5] I. Goodfellow, Y. Bengio, A. Courville, Deep Learning, MIT Press, 2016.
[6] S. Minaee, et al., Large language models: A survey, arXiv (2025), 2402.06196v3.
[7] M.A. Nielsen and I.L. Chuang, Quantum computation and quantum information, Cambridge University Press, Cambridge, 2001.
[8] V. Paulsen, Completely Bounded Maps and Operator Al-gebras, Cambridge University Press, Cambridge, 2002.
orem (cf.[8, Theorem 7.5]),  $ E_{\Phi} $  extends to a completely positive operator E in  $ \mathcal{L}(\mathbb{H}) $ , i.e., a quantum operation in H (note that E is not necessarily unique). By the construction, E satisfies the required condition.
[9] M. Reed, B. Simon, Method of Modern Mathematical Physics, Vol. I, Academic Press, San Diego, 1980.
[10] K. Sharma, M. Cerezo, L. Cincio, P.J. Coles, Trainability of dissipative perceptron-based quantum neural networks, Physical Review Letters 128 (2022), 180505: 1-7.
[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, L. Kaiser, I. Polosukhin, Attention is all you need, Advances in Neural Information Processing Systems, 30 (2017), 5998-6008.
[12] C.J. Villas-Boas, C.E. Máximo, P.J. Paulino, R.P. Bachelard, G. Rempe, Bright and dark states of light: The quantum origin of classical interference, Physical Review Letters 134 (2025), 133603: 1-6.
[13] J. Vuckovic, A. Baratin, R.T. Combes, A mathematical theory of attention, arXiv (2020), 2007.02876.
[14] Y. Zhang, et al., Tensor product attention is all you need, arXiv (2025), 2501.06425.
[15] W.X. Zhao, et al., A survey of large language models, arXiv (2025), 2303.18223v16.
This version posted 2025-05-27.
